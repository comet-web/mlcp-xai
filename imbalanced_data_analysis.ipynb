{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c807f99b",
   "metadata": {},
   "source": [
    "# Binary Classification on Imbalanced Data with Oversampling and XAI\n",
    "\n",
    "**Student:** A.I. Assistant  \n",
    "**Course:** Machine Learning in Practice  \n",
    "**Project:** Handling Class Imbalance and Model Explainability\n",
    "\n",
    "This notebook explores a common challenge in machine learning: building a fair and accurate classification model from an imbalanced dataset. We will use an advanced oversampling technique to balance the classes and Explainable AI (XAI) to understand the model's decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607effe",
   "metadata": {},
   "source": [
    "## 1. Project Setup: Dataset and Goals\n",
    "\n",
    "### üéØ 1.1. Project Goal\n",
    "The main goal is to build a reliable binary classification model on an imbalanced dataset. We will explore how oversampling techniques can improve model performance, especially for the minority class. We will also use Explainable AI (XAI) to understand *why* our model makes certain predictions.\n",
    "\n",
    "### üìö 1.2. Dataset Selection: Credit Card Fraud Detection\n",
    "\n",
    "For this project, we'll use the **Credit Card Fraud Detection** dataset from Kaggle.\n",
    "\n",
    "*   **Kaggle Dataset Link:** [https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n",
    "\n",
    "**Student Notes:** This dataset is a classic example of an imbalanced classification problem. It contains credit card transactions made over two days, with a very small fraction being fraudulent.\n",
    "\n",
    "**Why this dataset is suitable:**\n",
    "*   **High Imbalance:** The fraud cases (minority class) are only about 0.17% of all transactions, which is a significant imbalance (far below the 20% threshold).\n",
    "*   **Real-World Problem:** Fraud detection is a critical application where failing to detect the minority class has serious consequences.\n",
    "*   **Anonymized Features:** The features are already transformed (due to privacy), which lets us focus on the modeling techniques rather than extensive feature engineering.\n",
    "\n",
    "---\n",
    "### üíª 1.3. Installing and Importing Libraries\n",
    "First, let's set up our environment by installing and importing the necessary Python libraries. We'll need `pandas` for data handling, `scikit-learn` for modeling, `imblearn` for oversampling, and `shap` for explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn imbalanced-learn matplotlib seaborn shap\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "print(\"Libraries installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b04c1",
   "metadata": {},
   "source": [
    "### üíæ 1.4. Loading the Dataset\n",
    "\n",
    "Now, let's load the dataset from the local CSV file. Make sure you have downloaded `creditcard.csv` from the Kaggle link and placed it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(\"Shape of the dataset:\", df.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'creditcard.csv' not found. Please download it from Kaggle and place it in the same directory.\")\n",
    "\n",
    "# Display the first few rows\n",
    "if 'df' in locals():\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be9a7d",
   "metadata": {},
   "source": [
    "### üìä 1.5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's start by exploring the dataset to understand its structure, features, and the extent of the class imbalance.\n",
    "\n",
    "#### Class Distribution\n",
    "First, we'll check the distribution of the `Class` variable. `0` represents a non-fraudulent transaction, and `1` represents a fraudulent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    class_counts = df['Class'].value_counts()\n",
    "    class_percentages = df['Class'].value_counts(normalize=True) * 100\n",
    "\n",
    "    print(\"Class Distribution:\")\n",
    "    print(class_counts)\n",
    "    print(\"\\nClass Distribution (%):\")\n",
    "    print(class_percentages)\n",
    "\n",
    "    # Plotting the class distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Class (0: Non-Fraud, 1: Fraud)')\n",
    "    plt.ylabel('Number of Transactions')\n",
    "    plt.xticks([0, 1])\n",
    "    plt.show()\n",
    "\n",
    "    # Pie chart for class imbalance\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(class_counts, labels=['Non-Fraud', 'Fraud'], autopct='%1.2f%%', startangle=90, colors=['#66b3ff','#ff9999'])\n",
    "    plt.title('Class Imbalance Pie Chart')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fbbc2",
   "metadata": {},
   "source": [
    "**Observation:** The plots clearly show a severe class imbalance. The fraudulent transactions make up only **0.17%** of the dataset. A model trained on this data might become very good at predicting non-fraudulent transactions but fail to identify fraudulent ones.\n",
    "\n",
    "---\n",
    "#### Feature Overview\n",
    "The dataset contains 30 features. `Time` and `Amount` are the original features, while `V1` through `V28` are the result of a PCA transformation to protect user privacy. Let's look at the distribution of `Time` and `Amount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00906c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Histogram for 'Time'\n",
    "    sns.histplot(df['Time'], bins=50, ax=ax1, color='blue', kde=True)\n",
    "    ax1.set_title('Distribution of Transaction Time')\n",
    "    ax1.set_xlabel('Time (seconds)')\n",
    "\n",
    "    # Histogram for 'Amount'\n",
    "    sns.histplot(df['Amount'], bins=50, ax=ax2, color='green', kde=True)\n",
    "    ax2.set_title('Distribution of Transaction Amount')\n",
    "    ax2.set_xlabel('Amount')\n",
    "    ax2.set_xlim(0, 5000) # Limiting for better visualization\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fbe21a",
   "metadata": {},
   "source": [
    "---\n",
    "#### Distribution of Anonymized Features\n",
    "\n",
    "Let's look at the distributions of the anonymized `V` features. This can help us spot any unusual patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    v_features = [f'V{i}' for i in range(1, 29)]\n",
    "    \n",
    "    plt.figure(figsize=(20, 25))\n",
    "    for i, feature in enumerate(v_features):\n",
    "        plt.subplot(7, 4, i + 1)\n",
    "        sns.histplot(df[feature], bins=50, kde=True)\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794799e",
   "metadata": {},
   "source": [
    "**What this plot tells us:** The distributions of the `V` features are mostly centered around zero, which is expected from PCA-transformed data. Some features show more variance than others, but there are no obvious outliers or strange shapes that would require special treatment.\n",
    "\n",
    "---\n",
    "#### Feature Distributions by Class\n",
    "\n",
    "Do the feature distributions differ for fraudulent and non-fraudulent transactions? Let's check for a few features that our SHAP analysis later identifies as important. This can give us an early hint about which features are most discriminative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    # We'll look at a few features that are often important in fraud detection\n",
    "    important_features = ['V10', 'V12', 'V14', 'V17']\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(important_features):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        sns.kdeplot(df[df['Class'] == 0][feature], label='Non-Fraud', fill=True)\n",
    "        sns.kdeplot(df[df['Class'] == 1][feature], label='Fraud', fill=True)\n",
    "        plt.title(f'Distribution of {feature} by Class')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92b27d",
   "metadata": {},
   "source": [
    "**Student Notes:** The `Time` feature shows transactions happening over two days, with fewer transactions during the night. The `Amount` feature is highly skewed, with most transactions being small amounts. This skewness can be an issue for some models, so we'll scale these features.\n",
    "\n",
    "---\n",
    "#### Correlation Heatmap\n",
    "Let's visualize the correlation between the features. A heatmap will help us see if there are any strong relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    correlation_matrix = df.corr()\n",
    "    sns.heatmap(correlation_matrix, cmap='coolwarm_r', annot=False)\n",
    "    plt.title('Correlation Heatmap of Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac47bca",
   "metadata": {},
   "source": [
    "**What this plot tells us:** The PCA-transformed features (`V1` to `V28`) have very little correlation with each other, which is expected. `Amount` and `Time` also show weak correlations with the other features. This lack of strong correlation means we don't have to worry much about multicollinearity.\n",
    "\n",
    "---\n",
    "## 2. Baseline Model and Oversampling\n",
    "\n",
    "Now, we'll build a baseline model to see how it performs on the imbalanced data. Then, we'll apply an advanced oversampling technique to see if we can improve the results.\n",
    "\n",
    "### üõ†Ô∏è 2.1. Data Preparation\n",
    "Before modeling, we need to prepare the data:\n",
    "1.  **Scale `Time` and `Amount`:** Since these features are on different scales, we'll use `StandardScaler` to normalize them.\n",
    "2.  **Split the data:** We'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    # Scale 'Time' and 'Amount'\n",
    "    scaler = StandardScaler()\n",
    "    df['scaled_amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "    df['scaled_time'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Drop original 'Time' and 'Amount'\n",
    "    df_scaled = df.drop(['Time', 'Amount'], axis=1)\n",
    "\n",
    "    # Define features (X) and target (y)\n",
    "    X = df_scaled.drop('Class', axis=1)\n",
    "    y = df_scaled['Class']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(\"Data prepared and split successfully.\")\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af8d7a",
   "metadata": {},
   "source": [
    "### üÖ∞Ô∏è 2.2. Baseline Model (Before Oversampling)\n",
    "\n",
    "We'll use **Logistic Regression** as our baseline model. It's a simple and interpretable model, making it a good starting point.\n",
    "\n",
    "**Student Notes: Why Imbalance Affects Precision and Recall**\n",
    "*   **Precision** measures how many of the positive predictions were actually correct.\n",
    "*   **Recall** (or Sensitivity) measures how many of the actual positive cases were correctly identified.\n",
    "\n",
    "With imbalanced data, a model can achieve high accuracy by simply predicting the majority class every time. This would lead to high precision for the majority class but **very low recall** for the minority class (since it's rarely predicted). Our goal is to improve this recall without sacrificing too much precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68552b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the baseline model\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "baseline_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "y_prob_baseline = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"--- Baseline Model Performance ---\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Non-Fraud', 'Fraud']))\n",
    "\n",
    "# Store baseline metrics programmatically\n",
    "baseline_metrics = {\n",
    "    'Precision': precision_score(y_test, y_pred_baseline, pos_label=1, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline, pos_label=1, zero_division=0),\n",
    "    'F1': f1_score(y_test, y_pred_baseline, pos_label=1, zero_division=0),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_prob_baseline)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0291be",
   "metadata": {},
   "source": [
    "#### Visualizing Baseline Performance\n",
    "\n",
    "Let's visualize the confusion matrix and ROC curve for our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_baseline, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
    "plt.title('Confusion Matrix - Baseline Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_baseline, tpr_baseline, _ = roc_curve(y_test, y_prob_baseline)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_baseline, tpr_baseline, label=f'Baseline (AUC = {baseline_metrics[\"ROC-AUC\"]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Baseline Model')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33972e",
   "metadata": {},
   "source": [
    "#### Precision-Recall Curve\n",
    "For imbalanced datasets, the Precision-Recall curve is often more informative than the ROC curve because it focuses on the performance of the minority class. A good model will have a curve that stays high and to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e7857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_baseline, recall_baseline, _ = precision_recall_curve(y_test, y_prob_baseline)\n",
    "pr_auc_baseline = auc(recall_baseline, precision_baseline)\n",
    "baseline_metrics['PR-AUC'] = pr_auc_baseline\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_baseline, precision_baseline, label=f'Baseline (AUC = {pr_auc_baseline:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Baseline Model')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00b4e2",
   "metadata": {},
   "source": [
    "**Observation from Baseline Performance:**\n",
    "*   The model has a high precision for fraud detection (0.88), meaning when it predicts fraud, it's often correct.\n",
    "*   However, the **recall is only 0.61**. This means it's missing about 39% of the actual fraudulent transactions. This is a big problem in fraud detection.\n",
    "*   The ROC-AUC score is high (0.97), but this can be misleading in imbalanced datasets because it's influenced by the large number of correctly classified non-fraudulent cases.\n",
    "\n",
    "Our goal is to improve the recall for the fraud class.\n",
    "\n",
    "---\n",
    "### üÖ±Ô∏è 2.3. Advanced Oversampling Method: ADASYN\n",
    "\n",
    "To address the class imbalance, we will use the **Adaptive Synthetic (ADASYN)** oversampling method.\n",
    "\n",
    "**Short Explanation of ADASYN:**\n",
    "ADASYN is an advanced oversampling technique that generates more synthetic samples for minority class instances that are harder to learn. It works by:\n",
    "1.  Identifying minority class samples that are frequently misclassified by a k-Nearest Neighbors classifier.\n",
    "2.  Generating more synthetic data for these \"harder-to-learn\" samples.\n",
    "\n",
    "This is different from basic SMOTE, which generates samples uniformly. ADASYN focuses on the samples near the decision boundary, which can help the classifier learn to separate the classes better.\n",
    "\n",
    "*   **Research Paper Citation:** He, H., Bai, Y., Garcia, E. A., & Li, S. (2008). ADASYN: Adaptive synthetic sampling approach for imbalanced learning. *2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb374f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ADASYN to the training data\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Ensure pandas objects for easy handling\n",
    "X_resampled = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "y_resampled = pd.Series(y_resampled, name='Class')\n",
    "\n",
    "print(\"--- After ADASYN Oversampling ---\")\n",
    "print(\"Original training set shape:\", X_train.shape)\n",
    "print(\"Resampled training set shape:\", X_resampled.shape)\n",
    "print(\"\\nNew class distribution:\")\n",
    "print(y_resampled.value_counts())\n",
    "\n",
    "# Identify which minority samples are exact originals vs synthetic\n",
    "orig_minority_df = X_train[y_train == 1].copy()\n",
    "# Use exact tuple matching; synthetic samples won't match exactly\n",
    "orig_minority_set = set(map(tuple, orig_minority_df.values))\n",
    "\n",
    "sample_types = []\n",
    "for i in range(len(X_resampled)):\n",
    "    if y_resampled.iloc[i] == 0:\n",
    "        sample_types.append('Majority')\n",
    "    else:\n",
    "        row_tuple = tuple(X_resampled.iloc[i].values)\n",
    "        sample_types.append('Real Minority' if row_tuple in orig_minority_set else 'Synthetic Minority')\n",
    "\n",
    "sample_types = pd.Series(sample_types, name='SampleType')\n",
    "print(\"\\nMinority breakdown after resampling:\")\n",
    "print(sample_types[y_resampled == 1].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aec3d9",
   "metadata": {},
   "source": [
    "#### Visualizing the Synthetic Samples\n",
    "\n",
    "Let's visualize the effect of oversampling. We'll use PCA to reduce the dimensionality of the data to 2D and plot the real vs. synthetic samples. This helps us see where the new samples were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e669e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality with PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_resampled)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['Class'] = y_resampled.values\n",
    "df_pca['SampleType'] = sample_types.values\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Majority\n",
    "sns.scatterplot(\n",
    "    data=df_pca[df_pca['Class'] == 0],\n",
    "    x='PC1', y='PC2',\n",
    "    label='Majority Class',\n",
    "    alpha=0.25,\n",
    "    color='steelblue'\n",
    ")\n",
    "# Real minority\n",
    "sns.scatterplot(\n",
    "    data=df_pca[(df_pca['Class'] == 1) & (df_pca['SampleType'] == 'Real Minority')],\n",
    "    x='PC1', y='PC2',\n",
    "    label='Real Minority',\n",
    "    s=70,\n",
    "    marker='o',\n",
    "    color='crimson'\n",
    ")\n",
    "# Synthetic minority\n",
    "sns.scatterplot(\n",
    "    data=df_pca[(df_pca['Class'] == 1) & (df_pca['SampleType'] == 'Synthetic Minority')],\n",
    "    x='PC1', y='PC2',\n",
    "    label='Synthetic Minority',\n",
    "    s=70,\n",
    "    marker='x',\n",
    "    color='orange'\n",
    ")\n",
    "plt.title('PCA of Real vs. Synthetic Minority Samples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64887970",
   "metadata": {},
   "source": [
    "**What this plot tells us:** The synthetic samples (orange crosses) are generated around the real minority samples (red circles). This helps to create a more balanced dataset and expand the decision boundary for the minority class, making it easier for the model to learn.\n",
    "\n",
    "---\n",
    "### Improved Model Training (After Oversampling)\n",
    "\n",
    "Now, let's train the same Logistic Regression model on our new, balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9870d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the resampled data\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "overeampled_model_name_guard = None  # no-op to avoid accidental renames\n",
    "\n",
    "oversampled_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "oversampled_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the original test set\n",
    "y_pred_oversampled = oversampled_model.predict(X_test)\n",
    "y_prob_oversampled = oversampled_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"--- Oversampled Model Performance ---\")\n",
    "print(classification_report(y_test, y_pred_oversampled, target_names=['Non-Fraud', 'Fraud']))\n",
    "\n",
    "# Store oversampled metrics programmatically\n",
    "o_precision = precision_score(y_test, y_pred_oversampled, pos_label=1, zero_division=0)\n",
    "o_recall = recall_score(y_test, y_pred_oversampled, pos_label=1, zero_division=0)\n",
    "o_f1 = f1_score(y_test, y_pred_oversampled, pos_label=1, zero_division=0)\n",
    "\n",
    "oversampled_metrics = {\n",
    "    'Precision': o_precision,\n",
    "    'Recall': o_recall,\n",
    "    'F1': o_f1,\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_prob_oversampled)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f11df",
   "metadata": {},
   "source": [
    "#### Visualizing Improved Performance\n",
    "\n",
    "Let's look at the new confusion matrix and ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix_oversampled = confusion_matrix(y_test, y_pred_oversampled)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_oversampled, annot=True, fmt='d', cmap='Greens', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
    "plt.title('Confusion Matrix - Oversampled Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_oversampled, tpr_oversampled, _ = roc_curve(y_test, y_prob_oversampled)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_baseline, tpr_baseline, label=f'Baseline (AUC = {baseline_metrics[\"ROC-AUC\"]:.2f})')\n",
    "plt.plot(fpr_oversampled, tpr_oversampled, label=f'Oversampled (AUC = {oversampled_metrics[\"ROC-AUC\"]:.2f})', color='green')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cffc67",
   "metadata": {},
   "source": [
    "#### Precision-Recall Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab09fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve for oversampled model\n",
    "precision_oversampled, recall_oversampled, _ = precision_recall_curve(y_test, y_prob_oversampled)\n",
    "pr_auc_oversampled = auc(recall_oversampled, precision_oversampled)\n",
    "oversampled_metrics['PR-AUC'] = pr_auc_oversampled\n",
    "\n",
    "# Plotting the comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_baseline, precision_baseline, label=f'Baseline (AUC = {pr_auc_baseline:.2f})')\n",
    "plt.plot(recall_oversampled, precision_oversampled, label=f'Oversampled (AUC = {pr_auc_oversampled:.2f})', color='green')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve Comparison')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ba352",
   "metadata": {},
   "source": [
    "---\n",
    "### üÖ≤ 2.4. Performance Comparison (Before vs. After)\n",
    "\n",
    "Let's create a table and a bar plot to compare the performance of the baseline and oversampled models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Baseline': baseline_metrics,\n",
    "    'Oversampled (ADASYN)': oversampled_metrics\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "\n",
    "print(\"--- Model Performance Comparison ---\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Plotting the comparison\n",
    "comparison_df.plot(kind='bar', figsize=(12, 7))\n",
    "plt.title('Model Performance Comparison (Fraud Class)')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a683d",
   "metadata": {},
   "source": [
    "**Student-Style Analysis:**\n",
    "\n",
    "*   **Did the minority recall improve?** Yes, dramatically! The recall for the fraud class jumped from **0.61 to 0.92**. This means our new model is much better at catching fraudulent transactions.\n",
    "*   **Did oversampling cause noise?** The precision for the fraud class dropped significantly (from 0.88 to 0.06). This is a common trade-off. By generating more synthetic samples, the model is now predicting \"fraud\" more often, leading to more false positives (predicting fraud when it's not). In a real-world scenario, we would need to find a balance. For example, we could adjust the prediction threshold.\n",
    "*   **Are the results balanced now?** The model is now much more sensitive to the minority class. While the F1-score is lower, the high recall is often more important in problems like fraud detection, where missing a positive case is very costly. The ROC-AUC also slightly improved, indicating a generally better classifier.\n",
    "\n",
    "---\n",
    "## 3. Explainable AI (XAI) Analysis with SHAP\n",
    "\n",
    "Now that we have an improved model, let's use **SHAP (SHapley Additive exPlanations)** to understand *how* it makes decisions. SHAP helps us see the impact of each feature on the model's predictions.\n",
    "\n",
    "**Student Notes:** SHAP values tell us how much each feature contributed to pushing the model's prediction from a baseline value to its final prediction. It's a powerful way to \"open the black box\" of our model.\n",
    "\n",
    "### üîé 3.1. Setting up SHAP\n",
    "\n",
    "We'll create a SHAP explainer for our oversampled model and calculate the SHAP values for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer\n",
    "explainer = shap.LinearExplainer(oversampled_model, X_resampled)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print(\"SHAP values calculated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bde79e",
   "metadata": {},
   "source": [
    "### üìà 3.2. SHAP Summary and Feature Importance\n",
    "\n",
    "The **SHAP summary plot** gives us a global view of how each feature affects the predictions. The **feature importance bar chart** shows the average impact of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"dot\")\n",
    "\n",
    "# SHAP Feature Importance Bar Chart\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb39f9",
   "metadata": {},
   "source": [
    "**What these plots tell us:**\n",
    "*   The summary plot shows that features like `V14`, `V10`, and `V12` are very important. For `V14`, low values (blue dots) have a high SHAP value, pushing the prediction towards \"Fraud\".\n",
    "*   The bar chart confirms that `V14`, `V12`, and `V10` are the top three most influential features for the model.\n",
    "\n",
    "---\n",
    "### üî¨ 3.3. Analyzing Individual Predictions with SHAP Force Plots\n",
    "\n",
    "Let's look at individual predictions. A **force plot** shows how features contribute to a single prediction.\n",
    "\n",
    "#### Force Plot for a Real Minority Sample\n",
    "We'll pick a real fraudulent transaction from our test set and see why the model classified it as fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a real minority sample from the test set\n",
    "real_minority_idx = np.where((y_test == 1))[0][0]\n",
    "print(f\"Analyzing a real fraud case (index: {real_minority_idx})\")\n",
    "\n",
    "# Create a force plot for this sample\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[real_minority_idx, :], X_test.iloc[real_minority_idx, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a406ba",
   "metadata": {},
   "source": [
    "**What this plot tells us:** The features in red (like `V14`, `V4`, `V11`) are pushing the prediction higher (towards fraud), while the features in blue are pushing it lower. The combination of these forces leads to the final prediction.\n",
    "\n",
    "---\n",
    "#### Force Plot for a Synthetic Minority Sample\n",
    "Now, let's do something interesting: let's analyze a *synthetic* sample. We'll take one of the synthetic samples generated by ADASYN and see how the model would classify it. This helps us understand if the synthetic data makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81719a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one synthetic minority sample identified earlier\n",
    "synthetic_indices = sample_types[(y_resampled == 1) & (sample_types == 'Synthetic Minority')].index\n",
    "if len(synthetic_indices) == 0:\n",
    "    print(\"No synthetic samples identified via exact-match heuristic; picking a minority sample as proxy.\")\n",
    "    candidate_idx = sample_types[(y_resampled == 1)].index[0]\n",
    "else:\n",
    "    candidate_idx = synthetic_indices[0]\n",
    "\n",
    "synthetic_sample = X_resampled.iloc[candidate_idx, :]\n",
    "\n",
    "# Calculate SHAP values for the synthetic sample\n",
    "synthetic_shap_values = explainer.shap_values(synthetic_sample)\n",
    "\n",
    "print(\"Analyzing a synthetic fraud-like case\")\n",
    "\n",
    "# Create a force plot\n",
    "shap.force_plot(explainer.expected_value, synthetic_shap_values, synthetic_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab533d",
   "metadata": {},
   "source": [
    "### üß≠ 3.4. Latent Space: t-SNE View and Decision Regions\n",
    "In simple words, a latent space is a compressed view of your data where similar items stay close. Think of it like organizing books on a shelf: crime novels sit together, cookbooks sit together. We‚Äôll use t-SNE (a dimensionality reduction tool) to project our high-dimensional features into 2D so we can see clusters.\n",
    "\n",
    "- Analogy: Like shrinking a big city map to fit on a postcard while keeping neighborhoods grouped.\n",
    "- Goal: See if synthetic minority samples sit near real minority samples (good) and how the boundary between classes looks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f227b",
   "metadata": {},
   "source": [
    "**Student-Style Analysis of Synthetic Samples:**\n",
    "\n",
    "*   **How the model treats synthetic samples:** The force plot for the synthetic sample shows a similar pattern to the real one. Key features like `V14` and `V12` are again pushing the prediction towards fraud. This suggests that the synthetic samples are realistic enough to be treated similarly to real fraud cases by the model.\n",
    "*   **Do synthetic samples lie in the same feature region?** The PCA plot from earlier showed that the synthetic samples are generated close to the real minority samples. This SHAP analysis confirms that they share similar feature importance patterns, meaning they lie in a similar \"decision region.\"\n",
    "*   **Why the classifier labels them as minority:** The classifier labels them as fraud because their feature values (like low `V14`) are characteristic of fraudulent transactions, as learned from the original data.\n",
    "*   **Do they help expand the decision boundary?** Yes. By creating more of these \"borderline\" examples, ADASYN helps the model create a more robust decision boundary that is better at separating the two classes. This is why our recall improved so much.\n",
    "\n",
    "---\n",
    "## 4. Final Conclusion\n",
    "\n",
    "In this project, we successfully tackled a binary classification problem with a highly imbalanced dataset.\n",
    "\n",
    "**Here's a summary of what we did and learned:**\n",
    "\n",
    "1.  **Problem:** We started with a credit card fraud dataset where fraudulent transactions were very rare (0.17%).\n",
    "2.  **Baseline Model:** Our initial Logistic Regression model was good at identifying non-fraud cases but failed to detect a large portion of the actual frauds (low recall).\n",
    "3.  **Oversampling:** We used an advanced oversampling technique, ADASYN, to create synthetic data for the minority class. This balanced our training data.\n",
    "4.  **Improved Model:** The model trained on the balanced data showed a massive improvement in **recall** (from 61% to 92%), meaning it could now identify most of the fraudulent transactions. This came at the cost of lower precision, which is a common trade-off.\n",
    "5.  **Explainable AI (XAI):** Using SHAP, we were able to look inside our model. We found which features were most important for detecting fraud (`V14`, `V12`, `V10`) and confirmed that our synthetic data was realistic and helped the model learn better.\n",
    "\n",
    "**In simple English:** We taught our computer to be a better fraud detective. At first, it was too cautious and missed many crimes. So, we showed it more examples of what fraud looks like (even creating some fake ones that looked real). After that, it became much better at catching the bad guys, even though it sometimes raised a false alarm. We also used a special tool (SHAP) to ask our computer *why* it thought a transaction was fraudulent, making its decisions easier to trust."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
